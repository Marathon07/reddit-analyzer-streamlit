# ----- BEGINNING OF reddit_analyzer_cloud_app_v14_syntax_fix_2.py -----
# Final Cloud Version v14 - Fixed ANOTHER SyntaxError in generate_ai_response else block

import streamlit as st
import praw
import json
from datetime import datetime, time, date # Import date explicitly
import pandas as pd
import time as py_time
import pytz
import os

# Attempt to import AI libraries
try:
    import google.generativeai as genai
    google_ai_available = True
except ImportError:
    google_ai_available = False

try:
    import openai
    openai_available = True
except ImportError:
    openai_available = False

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="Reddit åˆ†æå™¨ + AI å¯¹è¯ (å¤šæ¨¡å‹)", layout="wide", initial_sidebar_state="expanded")

# --- æ·»åŠ å›¾æ ‡ ---
st.title(":mag: Reddit åˆ†æå™¨ + AI å¯¹è¯ (å¤šæ¨¡å‹é€‰æ‹©) :robot_face:")
st.caption("æŠ“å– Reddit è®¨è®ºï¼Œå¹¶é€šè¿‡å¤šè½®å¯¹è¯ä¸æ‰€é€‰ AI æ¨¡å‹è¿›è¡Œåˆ†æ")

# === Helper Function to Format Chat History for OpenAI API style ===
def format_history_for_openai_style(chat_history_internal):
    """Converts internal chat history (role/parts) to OpenAI messages format (role/content)."""
    openai_messages = []
    for message in chat_history_internal:
        role = message.get("role")
        content = message.get("parts", [""])[0]
        mapped_role = "assistant" if role == "model" else role
        if mapped_role in ["system", "user", "assistant"]:
             openai_messages.append({"role": mapped_role, "content": content})
    return openai_messages

# === Unified AI å…³é”®è¯å¤„ç†å‡½æ•° ===
def get_english_keywords_for_query(api_key, user_query, selected_model_details, client_instance):
    """æ ¹æ®é€‰æ‹©çš„æ¨¡å‹å¤„ç†å…³é”®è¯ (æ”¯æŒ Gemini å’Œ Qwen)"""
    provider = selected_model_details.get("provider")
    model_id = selected_model_details.get("model_id")

    if not api_key or not client_instance:
        st.warning(f"æœªæœ‰æ•ˆé…ç½® {provider} Key æˆ–å®¢æˆ·ç«¯ï¼Œæ— æ³•è‡ªåŠ¨å¤„ç†ä¸­æ–‡å…³é”®è¯ã€‚")
        return user_query
    if not user_query.strip():
         st.warning("è¾“å…¥çš„å…³é”®è¯ä¸ºç©ºï¼Œæ— æ³•å¤„ç†ã€‚")
         return ""
    if not model_id:
        st.error("æœªæä¾›æœ‰æ•ˆçš„æ¨¡å‹ IDã€‚")
        return user_query

    reddit_query = user_query # Default fallback
    processed_query = None # Initialize to track if processing happened

    try:
        if provider == "Gemini":
            if not google_ai_available: raise ImportError("Google Generative AI library not loaded.")
            model = client_instance.GenerativeModel(model_id)
            prompt = f"""
            Process the user query "{user_query}" for a Reddit keyword search.
            1. If the query is in Chinese, translate its core meaning to a concise English keyword/phrase and generate 2-3 related English terms.
            2. If the query is already English, use it as the primary term and generate 2-3 related English terms.
            3. **CRITICAL**: Return ONLY a comma-separated list of the final English terms, enclosed in parentheses. Example: (term1), (term2), (term3).
            4. **DO NOT** include any explanations, reasoning, language detection results, or any text other than the comma-separated list in parentheses.
            Example input "è€å¹´äºº æ¶ˆè´¹", Expected output ONLY: (elderly consumption), (senior spending), (aging population purchase), (geriatric cost)
            """
            response = model.generate_content(prompt)
            if hasattr(response, 'text') and response.text:
                 keywords_str = response.text.strip()
                 if keywords_str.startswith("(") and keywords_str.endswith(")") and "," in keywords_str:
                     keywords_list = [kw.strip() for kw in keywords_str.split(',') if kw.strip()]
                     if keywords_list:
                        processed_terms = [f"({kw.strip('()')})" for kw in keywords_list if kw.strip('()')]
                        processed_query = " OR ".join(processed_terms) if processed_terms else None
                        if processed_query: st.info(f"å·²ä½¿ç”¨ {model_id} (Gemini) å¤„ç†å…³é”®è¯: {processed_query}")
                        else: st.warning(f"{model_id} (Gemini) è§£æå…³é”®è¯åˆ—è¡¨åä¸ºç©º...");
                     else: st.warning(f"{model_id} (Gemini) æœªèƒ½ä»å“åº”ä¸­è§£æå‡ºå…³é”®è¯åˆ—è¡¨...");
                 else: st.warning(f"{model_id} (Gemini) è¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ: '{keywords_str[:100]}...'");
            else: st.warning(f"{model_id} (Gemini) æœªèƒ½å¤„ç†å…³é”®è¯");

        elif provider == "Qwen":
            if not openai_available: raise ImportError("OpenAI library (for Qwen) not loaded.")
            prompt_messages = [
                {"role": "system", "content": "You are an assistant that processes user queries for a Reddit keyword search. Analyze query, detect language. If Chinese, translate to English and generate 2-3 related English terms. If English, use original and generate 2-3 related terms. Return ONLY a comma-separated list like: (term1), (term2), (term3)"},
                {"role": "user", "content": user_query}
            ]
            response = client_instance.chat.completions.create(model=model_id, messages=prompt_messages, temperature=0.5, max_tokens=150)
            if response.choices and response.choices[0].message and response.choices[0].message.content:
                 keywords_str = response.choices[0].message.content.strip()
                 if "," in keywords_str and len(keywords_str) < 200: # Relaxed Qwen check
                     keywords_list = [kw.strip() for kw in keywords_str.split(',') if kw.strip()]
                     if keywords_list:
                        processed_terms = [f"({kw.strip('()')})" for kw in keywords_list if kw.strip('()')]
                        processed_query = " OR ".join(processed_terms) if processed_terms else None
                        if processed_query: st.info(f"å·²ä½¿ç”¨ {model_id} (Qwen) å¤„ç†å…³é”®è¯: {processed_query}")
                        else: st.warning(f"{model_id} (Qwen) è§£æå…³é”®è¯åˆ—è¡¨åä¸ºç©º...");
                     else: st.warning(f"{model_id} (Qwen) æœªèƒ½ä»å“åº”ä¸­è§£æå‡ºå…³é”®è¯åˆ—è¡¨...");
                 else: st.warning(f"{model_id} (Qwen) è¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ (ç¼ºå°‘é€—å·æˆ–è¿‡é•¿): '{keywords_str[:100]}...'");
            else: st.warning(f"{model_id} (Qwen) æœªèƒ½å¤„ç†å…³é”®è¯");
        else: st.error(f"æœªçŸ¥çš„æ¨¡å‹æä¾›å•†: {provider}")

        return processed_query if processed_query else user_query

    except Exception as e:
        st.error(f"è°ƒç”¨ {model_id} ({provider}) å¤„ç†å…³é”®è¯æ—¶å‡ºé”™: {e}")
        return user_query # Fallback


# === æ ¸å¿ƒæŠ“å–å‡½æ•° ===
# (å‡½æ•°å†…å®¹ä¸ V12 ç›¸åŒ)
def run_scraper(sub_name, query, limit, t_filter, get_comments, use_dates, start_dt, end_dt, reddit_client):
    """æ‰§è¡Œ Reddit æ•°æ®æŠ“å–çš„å‡½æ•°"""
    all_data = []
    status_area = st.empty()
    if not reddit_client: status_area.error("Reddit PRAW å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ã€‚"); return None
    try:
        status_area.info(f"æ­£åœ¨ r/{sub_name} ä¸­æœç´¢ '{query}' (åŸºæœ¬æ—¶é—´èŒƒå›´: {t_filter})..."); subreddit = reddit_client.subreddit(sub_name)
        if not query or not query.strip('()'): status_area.warning("æœç´¢æŸ¥è¯¢ä¸ºç©º..."); return []
        search_results = list(subreddit.search(query=query, sort='new', time_filter=t_filter, limit=limit)); status_area.info(f"åˆæ­¥æ‰¾åˆ° {len(search_results)} ä¸ªå¸–å­...")
        start_timestamp, end_timestamp = None, None
        if use_dates and start_dt and end_dt:
            try:
                if isinstance(start_dt, date) and isinstance(end_dt, date):
                     start_datetime_naive = datetime.combine(start_dt, time.min); end_datetime_naive = datetime.combine(end_dt, time.max)
                     utc_tz = pytz.utc; start_timestamp = utc_tz.localize(start_datetime_naive).timestamp(); end_timestamp = utc_tz.localize(end_datetime_naive).timestamp()
                     if start_timestamp > end_timestamp: st.warning("å¼€å§‹æ—¥æœŸæ™šäºç»“æŸæ—¥æœŸ..."); start_timestamp, end_timestamp = None, None
                     else: status_area.info(f"å°†åº”ç”¨æ—¥æœŸèŒƒå›´: {start_dt.strftime('%Y-%m-%d')} è‡³ {end_dt.strftime('%Y-%m-%d')}")
                else:
                     if use_specific_dates: st.warning("æ—¥æœŸè¾“å…¥æ— æ•ˆæˆ–æœªå®Œæ•´é€‰æ‹©..."); start_timestamp, end_timestamp = None, None
            except Exception as date_e: st.error(f"å¤„ç†æ—¥æœŸè¾“å…¥æ—¶å‡ºé”™: {date_e}"); start_timestamp, end_timestamp = None, None
        total_posts_processed_this_run, date_skipped_count = 0, 0; progress_bar = st.progress(0) if search_results else None
        for i, submission in enumerate(search_results):
            if progress_bar: progress_bar.progress((i + 1) / len(search_results))
            if start_timestamp and end_timestamp:
                if not (start_timestamp <= submission.created_utc <= end_timestamp): date_skipped_count += 1; continue
            status_area.info(f"æ­£åœ¨å¤„ç†å¸–å­ {i+1}/{len(search_results)}: {submission.id} - {submission.title[:30]}..."); post_info = {'id': submission.id, 'title': submission.title, 'selftext': submission.selftext, 'url': submission.url, 'score': submission.score, 'created_utc': submission.created_utc, 'comments': []}
            if get_comments:
                try:
                    submission.comment_sort = 'new'; submission.comments.replace_more(limit=0); comment_limit_per_post = 20; processed_comments = 0
                    for comment in submission.comments.list():
                        if processed_comments >= comment_limit_per_post: break
                        if hasattr(comment, 'body'): post_info['comments'].append({'id': comment.id, 'author': str(comment.author), 'body': comment.body, 'score': comment.score, 'created_utc': comment.created_utc, 'parent_id': str(comment.parent_id), 'depth': comment.depth}); processed_comments += 1
                except Exception as e: st.warning(f"å¤„ç†å¸–å­ {submission.id} è¯„è®ºæ—¶å‡ºé”™: {e}")
            all_data.append(post_info); total_posts_processed_this_run += 1; py_time.sleep(0.05)
        status_area.success(f"æŠ“å–å®Œæˆï¼æœ¬æ¬¡è¿è¡Œå¤„ç†äº† {total_posts_processed_this_run} ä¸ªå¸–å­ã€‚è·³è¿‡äº† {date_skipped_count} ä¸ªä¸ç¬¦åˆæ—¥æœŸèŒƒå›´çš„å¸–å­ã€‚"); return all_data
    except praw.exceptions.PRAWException as pe: status_area.error(f"Reddit API (PRAW) é”™è¯¯: {pe}"); return None
    except Exception as e: status_area.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {e}"); return None


# === Unified AI åˆ†æå‡½æ•° (FIXED Syntax Error AGAIN) ===
def generate_ai_response(api_key, chat_history, selected_model_details, client_instance):
    """æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è°ƒç”¨ API ç”ŸæˆèŠå¤©å›å¤ (æ”¯æŒ Gemini å’Œ Qwen)"""
    provider = selected_model_details.get("provider")
    model_id = selected_model_details.get("model_id")

    if not api_key or not client_instance:
        return f"é”™è¯¯ï¼šæœªæœ‰æ•ˆé…ç½® {provider} API Key æˆ–å®¢æˆ·ç«¯ã€‚"
    is_initial_qwen_call = (
            provider == "Qwen" and
            len(chat_history) == 2 and
            chat_history[0].get("role") == "system" and "content" in chat_history[0] and
            chat_history[1].get("role") == "user" and "content" in chat_history[1]
        )
    # Check normal history (role/parts) or initial Qwen call (role/content)
    if not is_initial_qwen_call and (not chat_history or not any(msg.get("parts") and msg["parts"][0] for msg in chat_history if msg.get("role") != "system")):
         return "é”™è¯¯ï¼šèŠå¤©å†å²ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚"

    if not model_id:
        return "é”™è¯¯ï¼šæœªæä¾›æœ‰æ•ˆçš„æ¨¡å‹ IDã€‚"

    try:
        if provider == "Gemini":
            if not google_ai_available: raise ImportError("Google Generative AI library not loaded.")
            model = client_instance.GenerativeModel(model_id)
            response = model.generate_content(chat_history)

            if hasattr(response, 'text') and response.text:
                 return response.text.strip()
            else:
                 # --- FIXED SYNTAX BLOCK ---
                 # Initialize feedback_text
                 feedback_text = "No specific feedback available."
                 # Safely try to get prompt_feedback
                 try:
                     if hasattr(response, 'prompt_feedback'):
                         feedback_text = str(response.prompt_feedback)
                 except Exception:
                     # Ignore potential errors during feedback retrieval/conversion
                     pass
                 # Now report the error on separate lines
                 st.error(f"{model_id} (Gemini) API è¿”å›äº†å“åº”ï¼Œä½†æ— æ³•æå–æ–‡æœ¬ã€‚åé¦ˆ: {feedback_text}")
                 return f"é”™è¯¯ï¼š{model_id} (Gemini) API è¿”å›æ— æ•ˆå“åº”ã€‚åé¦ˆ: {feedback_text}"
                 # --- END FIXED SYNTAX BLOCK ---

        elif provider == "Qwen":
            if not openai_available: raise ImportError("OpenAI library (for Qwen) not loaded.")
            # Format history correctly based on whether it's the initial call or not
            if is_initial_qwen_call:
                 formatted_history = chat_history # Use the pre-formatted system/user messages
            else:
                 formatted_history = format_history_for_openai_style(chat_history) # Convert subsequent history

            if not formatted_history: return "é”™è¯¯ï¼šå¤„ç†åçš„èŠå¤©å†å²ä¸ºç©ºã€‚"

            response = client_instance.chat.completions.create(model=model_id, messages=formatted_history, temperature=0.7)
            if response.choices and response.choices[0].message and response.choices[0].message.content:
                return response.choices[0].message.content.strip()
            else:
                st.error(f"{model_id} (Qwen) API æœªè¿”å›æœ‰æ•ˆå›å¤ã€‚")
                print(f"Qwen API Response Error Object: {response}") # Print full response for debugging
                return f"é”™è¯¯ï¼š{model_id} (Qwen) API æœªè¿”å›æœ‰æ•ˆå›å¤ã€‚"
        else:
             return f"é”™è¯¯ï¼šæœªçŸ¥çš„æ¨¡å‹æä¾›å•†: {provider}"

    except Exception as e:
        st.error(f"è°ƒç”¨ {model_id} ({provider}) API æ—¶å‡ºé”™: {e}")
        # Consider logging the full traceback here for deeper debugging if needed
        # import traceback
        # traceback.print_exc()
        return f"è°ƒç”¨ {model_id} ({provider}) API æ—¶å‡ºé”™: {type(e).__name__}"


# --- ä¾§è¾¹æ å®šä¹‰ ---
# (ä¿æŒ V12 çš„é€»è¾‘ä¸å˜)
with st.sidebar:
    st.header("å‚æ•°é…ç½®"); st.markdown("---")
    available_ai_models = {"Gemini 2.0 Flash (æ¨è)": {"provider": "Gemini", "model_id": "gemini-2.0-flash"}, "Qwen Turbo (æœ€æ–°)": {"provider": "Qwen", "model_id": "qwen-turbo-latest"}, "Qwen Plus (æœ€æ–°)": {"provider": "Qwen", "model_id": "qwen-plus-latest"}, "Gemini 2.5 Pro Exp (é™åˆ¶ä¸¥æ ¼!)": {"provider": "Gemini", "model_id": "gemini-2.5-pro-exp-03-25"}}
    model_display_names = list(available_ai_models.keys()); selected_model_display_name = st.selectbox("é€‰æ‹© AI æ¨¡å‹:", options=model_display_names, index=0, key="model_selector", help="é€‰æ‹© AI æ¨¡å‹ã€‚æ³¨æ„ Gemini Pro Exp å…è´¹å±‚çº§é™åˆ¶ä¸¥æ ¼ï¼")
    selected_model_details = available_ai_models[selected_model_display_name]; selected_provider = selected_model_details["provider"]; selected_model_id = selected_model_details["model_id"]
    st.markdown("---"); st.markdown(f"**å½“å‰é€‰æ‹©æ¨¡å‹:**"); st.markdown(f"- åç§°: `{selected_model_display_name}`"); st.markdown(f"- ID: `{selected_model_id}`"); st.markdown(f"- æä¾›å•†: `{selected_provider}`"); st.markdown("---")
    REDDIT_CLIENT_ID=st.secrets.get("REDDIT_CLIENT_ID"); REDDIT_CLIENT_SECRET=st.secrets.get("REDDIT_CLIENT_SECRET"); REDDIT_USER_AGENT=st.secrets.get("REDDIT_USER_AGENT"); reddit_client_instance=None
    if not REDDIT_CLIENT_ID or not REDDIT_CLIENT_SECRET or not REDDIT_USER_AGENT: st.error("é”™è¯¯ï¼šReddit API å‡­è¯æœªå®Œæ•´é…ç½®ã€‚")
    else:
        try: reddit_client_instance=praw.Reddit(client_id=REDDIT_CLIENT_ID,client_secret=REDDIT_CLIENT_SECRET,user_agent=REDDIT_USER_AGENT,read_only=True); st.success("Reddit API å‡­è¯å·²åŠ è½½ã€‚")
        except Exception as praw_e: st.error(f"åˆå§‹åŒ– Reddit PRAW å®¢æˆ·ç«¯æ—¶å‡ºé”™: {praw_e}"); reddit_client_instance=None
    GEMINI_API_KEY=st.secrets.get("GEMINI_API_KEY"); QWEN_API_KEY=st.secrets.get("DASHSCOPE_API_KEY"); gemini_configured=False; qwen_configured=False; ai_configured_successfully=False; active_client_instance=None; active_api_key=None
    if selected_provider=="Gemini":
        if google_ai_available and GEMINI_API_KEY:
            try: genai.configure(api_key=GEMINI_API_KEY); active_client_instance=genai; active_api_key=GEMINI_API_KEY; gemini_configured=True; ai_configured_successfully=True; st.success(f"Gemini (æ¨¡å‹: {selected_model_id}) é…ç½®æˆåŠŸï¼")
            except Exception as gemini_config_e: st.error(f"é…ç½® Gemini API æ—¶å‡ºé”™: {gemini_config_e}")
        elif google_ai_available: st.warning("æç¤ºï¼šé€‰æ‹©äº† Geminiï¼Œä½†æœªé…ç½® Gemini API Key...")
        else: st.error("Gemini åº“æœªåŠ è½½ã€‚")
    elif selected_provider=="Qwen":
        if openai_available and QWEN_API_KEY:
            try: qwen_base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"; active_client_instance=openai.OpenAI(api_key=QWEN_API_KEY,base_url=qwen_base_url); active_api_key=QWEN_API_KEY; qwen_configured=True; ai_configured_successfully=True; st.success(f"Qwen (æ¨¡å‹: {selected_model_id}) é…ç½®æˆåŠŸï¼")
            except Exception as qwen_config_e: st.error(f"é…ç½® Qwen (å…¼å®¹æ¨¡å¼) API æ—¶å‡ºé”™: {qwen_config_e}"); active_client_instance=None
        elif openai_available: st.warning("æç¤ºï¼šé€‰æ‹©äº† Qwenï¼Œä½†æœªé…ç½®å…¶ API Key (åç§°åº”ä¸º DASHSCOPE_API_KEY)...")
        else: st.error("OpenAI åº“æœªåŠ è½½ï¼Œæ— æ³•ä½¿ç”¨ Qwenã€‚")
    st.markdown("---")
    subreddit_name=st.text_input("Subreddit åç§°", "askSingapore", key="subreddit_input"); search_query_input=st.text_input("æœç´¢å…³é”®è¯ (æ”¯æŒä¸­æ–‡)", "amazon", key="query_input"); post_limit=st.number_input("æœ€å¤§å¸–å­æ•°é‡", min_value=1, max_value=1000, value=20, key="limit_input"); time_filter_options={"è¿‡å»ä¸€å°æ—¶":"hour","è¿‡å»ä¸€å¤©":"day","è¿‡å»ä¸€å‘¨":"week","è¿‡å»ä¸€æœˆ":"month","è¿‡å»ä¸€å¹´":"year","æ‰€æœ‰æ—¶é—´":"all"}; selected_time_label=st.selectbox("åŸºæœ¬æ—¶é—´èŒƒå›´:", options=list(time_filter_options.keys()), index=5, key="time_filter_select"); time_filter=time_filter_options[selected_time_label]
    start_scrape_button=st.button("å¼€å§‹æŠ“å– Reddit æ•°æ®", type="primary", icon="ğŸ”", disabled=(not reddit_client_instance), key="scrape_button"); st.markdown("---")
    use_specific_dates=st.checkbox("ä½¿ç”¨ç²¾ç¡®æ—¥æœŸèŒƒå›´è¿‡æ»¤", key="date_checkbox"); start_date_input=st.date_input("å¼€å§‹æ—¥æœŸ", value=None, disabled=not use_specific_dates, key="start_date"); end_date_input=st.date_input("ç»“æŸæ—¥æœŸ", value=None, disabled=not use_specific_dates, key="end_date"); st.caption("*æ—¥æœŸç­›é€‰åœ¨è·å–å¸–å­åè¿›è¡Œã€‚*"); st.markdown("---"); fetch_comments=st.checkbox("æŠ“å–è¯„è®º (é™åˆ¶æ¯ä¸ªå¸–å­ 20 æ¡)", value=True, key="comments_checkbox"); st.markdown("---")


# --- Initialize Session State ---
# (ä¿æŒ V12 çš„é€»è¾‘ä¸å˜)
if 'scraped_data' not in st.session_state: st.session_state.scraped_data = None
if 'chat_context' not in st.session_state: st.session_state.chat_context = {"history": [], "initial_input": None, "model_details_used": None}
if 'final_query_used' not in st.session_state: st.session_state.final_query_used = None

# --- Main Logic: Handle Scrape Button Click ---
# (ä¿æŒ V12 çš„é€»è¾‘ä¸å˜)
if start_scrape_button:
    st.session_state.scraped_data=None; st.session_state.chat_context={"history": [], "initial_input": None, "model_details_used": None}; st.session_state.final_query_used=None
    st.info(f"æ”¶åˆ°æŠ“å–ä»»åŠ¡ï¼Œæ­£åœ¨å¤„ç†å…³é”®è¯ '{search_query_input}'...")
    final_reddit_query = search_query_input
    if ai_configured_successfully and selected_model_details and active_api_key and active_client_instance:
        with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {selected_model_display_name} å¤„ç†/ç¿»è¯‘å…³é”®è¯..."): final_reddit_query=get_english_keywords_for_query(active_api_key, search_query_input, selected_model_details, active_client_instance)
    elif selected_provider: st.warning(f"{selected_provider} æœªæˆåŠŸé…ç½®ï¼Œå°†ä½¿ç”¨åŸå§‹å…³é”®è¯ã€‚")
    else: st.warning("æœªé€‰æ‹©æˆ–æœªåŠ è½½ AI æ¨¡å‹ï¼Œå°†ä½¿ç”¨åŸå§‹å…³é”®è¯ã€‚")
    st.session_state.final_query_used=final_reddit_query
    if st.session_state.final_query_used:
        st.info(f"æœ€ç»ˆç”¨äº Reddit æœç´¢çš„æŸ¥è¯¢è¯­å¥: '{st.session_state.final_query_used}'");
        with st.spinner(f"æ­£åœ¨ r/{subreddit_name} ä¸­æœç´¢å¹¶æŠ“å–æ•°æ®..."): scraped_data_result=run_scraper(subreddit_name, st.session_state.final_query_used, post_limit, time_filter, fetch_comments, use_specific_dates, start_date_input, end_date_input, reddit_client_instance)
        st.session_state.scraped_data=scraped_data_result; st.session_state.chat_context["model_details_used"]=selected_model_details if ai_configured_successfully else None; st.rerun()
    else: st.error("å…³é”®è¯å¤„ç†åä¸ºç©ºæˆ–å¤„ç†å¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œæœç´¢ã€‚"); st.stop()


# --- Display Scraped Results and AI Chat Interface ---
# (ä¿æŒ V12 çš„é€»è¾‘ä¸å˜)
if st.session_state.scraped_data is not None:
    model_details_for_this_session=st.session_state.chat_context.get("model_details_used"); model_display_name_for_this_session="AI"
    if model_details_for_this_session: model_display_name_for_this_session=next((name for name,details in available_ai_models.items() if details==model_details_for_this_session), model_details_for_this_session.get("model_id", "AI"))
    tab_preview, tab_ai_chat = st.tabs(["ğŸ“Š æ•°æ®é¢„è§ˆä¸ä¸‹è½½", f"ğŸ’¬ ä¸ {model_display_name_for_this_session} å¯¹è¯åˆ†æ"])
    with tab_preview: # Preview Tab Content
        st.subheader("æŠ“å–ç»“æœé¢„è§ˆ"); data_len=len(st.session_state.scraped_data) if isinstance(st.session_state.scraped_data, list) else 0; st.write(f"å…±æ‰¾åˆ° {data_len} ä¸ªç¬¦åˆæ¡ä»¶çš„å¸–å­è®°å½•ã€‚")
        if st.session_state.final_query_used: st.write(f"(ä½¿ç”¨çš„æœ€ç»ˆæœç´¢æŸ¥è¯¢: `{st.session_state.final_query_used}`) ")
        if not st.session_state.scraped_data or not isinstance(st.session_state.scraped_data, list) or data_len==0: st.warning("æ²¡æœ‰æœ‰æ•ˆçš„å¸–å­æ•°æ®å¯ä¾›é¢„è§ˆã€‚")
        else:
            try: # Preview Table
                posts_df_data=[];
                for post in st.session_state.scraped_data:
                    if isinstance(post,dict): post_preview={k: v for k,v in post.items() if k!='comments'}; post_preview['comment_count']=len(post.get('comments',[])); posts_df_data.append(post_preview)
                if posts_df_data:
                    posts_df=pd.DataFrame(posts_df_data);
                    if 'created_utc' in posts_df.columns:
                        try: posts_df['created_datetime_sgt']=pd.to_datetime(posts_df['created_utc'],unit='s',utc=True).dt.tz_convert('Asia/Singapore').dt.strftime('%Y-%m-%d %H:%M:%S SGT')
                        except Exception as tz_e: st.warning(f"æ— æ³•è½¬æ¢æ—¶é—´æˆ³: {tz_e}."); posts_df['created_datetime_sgt']=posts_df['created_utc']
                    preview_cols=['title','score','comment_count','created_datetime_sgt','url','id','selftext']; display_cols=[col for col in preview_cols if col in posts_df.columns]; st.dataframe(posts_df[display_cols], height=300)
                else: st.info("æ²’æœ‰å¸–å­æ•°æ®å¯ä¾›é¢„è§ˆã€‚")
            except Exception as e: st.error(f"æ— æ³•å°†ç»“æœæ ¼å¼åŒ–ä¸ºè¡¨æ ¼æ˜¾ç¤º: {e}")
            st.subheader("ä¸‹è½½æœ¬æ¬¡æŠ“å–çš„æ•°æ® (JSON)") # Download Button
            try: json_string=json.dumps(st.session_state.scraped_data, indent=4, ensure_ascii=False, default=str); timestamp_str=datetime.now().strftime("%Y%m%d_%H%M%S"); safe_query="".join(c if c.isalnum() else"_" for c in(st.session_state.final_query_used or search_query_input or"query"))[:50]; download_filename=f"reddit_{subreddit_name}_search_{safe_query}_{timestamp_str}.json"; st.download_button(label="ä¸‹è½½ JSON æ•°æ®", data=json_string, file_name=download_filename, mime="application/json", key="download_button")
            except Exception as e: st.error(f"ç”Ÿæˆ JSON ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    with tab_ai_chat: # AI Chat Tab
        st.subheader(f"ä¸ {model_display_name_for_this_session or 'AI'} å¯¹è¯åˆ†æ")
        if not model_details_for_this_session: st.error("æ²¡æœ‰æˆåŠŸé…ç½® AI Key æˆ–é€‰æ‹©æ¨¡å‹ç”¨äºæœ¬æ¬¡åˆ†æï¼Œæ— æ³•è¿›è¡Œ AI å¯¹è¯ã€‚")
        else:
             active_chat_client=None; active_chat_key=None; chat_provider=model_details_for_this_session["provider"]; chat_model_id=model_details_for_this_session["model_id"]
             if chat_provider=="Gemini":
                 if google_ai_available and GEMINI_API_KEY:
                     try: genai.configure(api_key=GEMINI_API_KEY); active_chat_client=genai; active_chat_key=GEMINI_API_KEY
                     except Exception as e: st.error(f"é‡æ–°é…ç½® Gemini å‡ºé”™: {e}")
             elif chat_provider=="Qwen":
                  if openai_available and QWEN_API_KEY:
                     try: qwen_base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"; active_chat_client=openai.OpenAI(api_key=QWEN_API_KEY, base_url=qwen_base_url); active_chat_key=QWEN_API_KEY
                     except Exception as e: st.error(f"é‡æ–°åˆå§‹åŒ– Qwen å®¢æˆ·ç«¯å‡ºé”™: {e}")
             if active_chat_client and active_chat_key: # Proceed only if client is ready
                if st.session_state.chat_context.get("initial_input"): # Show Initial Input Expander
                    with st.expander(f"æŸ¥çœ‹é¦–æ¬¡åˆ†æå‘é€ç»™ {model_display_name_for_this_session} çš„å®Œæ•´è¾“å…¥", expanded=False): st.text(st.session_state.chat_context["initial_input"])
                    st.markdown("---")
                if not st.session_state.chat_context["history"]: # Initial Analysis Section
                    query_for_prompt=st.session_state.final_query_used or search_query_input; default_initial_prompt = f"ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æŠ“å–çš„å…³äº '{query_for_prompt}' çš„ Reddit å¸–å­å’Œè¯„è®ºå†…å®¹ï¼Œæ€»ç»“ä¸»è¦çš„è®¨è®ºè§‚ç‚¹ã€ç”¨æˆ·æƒ…ç»ªã€å¯èƒ½å­˜åœ¨çš„æ¶ˆè´¹è¶‹åŠ¿ä»¥åŠäºšé©¬é€Šæ–°åŠ å¡ç«™çš„é€‰å“æ€è·¯ï¼Œå¹¶ç”¨è¡¨æ ¼åˆ—å‡ºæ¯ä¸ªé€‰å“ä¸Redditå†…å®¹çš„å…³è”æ€§ï¼Œå¹¶æŒ‰ç…§ç”¨æˆ·éœ€æ±‚çš„è¿«åˆ‡æ€§å¯¹é€‰å“è¿›è¡Œæ’åºã€‚(ä½¿ç”¨æ¨¡å‹: {model_display_name_for_this_session})"; st.write("**åˆæ­¥åˆ†ææŒ‡ä»¤ (å¯ç¼–è¾‘):**"); analysis_prompt_input = st.text_area(label="åˆæ­¥åˆ†ææŒ‡ä»¤", value=default_initial_prompt, height=150, key="initial_prompt_input_area", label_visibility="collapsed")
                    if st.button(f"ä½¿ç”¨ {model_display_name_for_this_session} è¿›è¡Œåˆæ­¥ AI åˆ†æ", icon=":material/auto_awesome:", key="initial_analysis_button"):
                        with st.spinner("æ­£åœ¨å‡†å¤‡åˆæ­¥åˆ†æè¯·æ±‚..."): # Prepare Initial Request
                            analysis_prompt=analysis_prompt_input; combined_text=""; max_chars=300000; current_chars=0; posts_included_count=0; limit_warning_shown=False
                            if isinstance(st.session_state.scraped_data, list):
                                for post in st.session_state.scraped_data: # Build combined_text
                                    if isinstance(post,dict): post_content=f"å¸–å­ ID: {post.get('id','')}\næ ‡é¢˜: {post.get('title','')}\næ­£æ–‡: {post.get('selftext','')}\n"; potential_len=current_chars+len(post_content);
                                    if potential_len>=max_chars:
                                        if not limit_warning_shown: st.warning(f"æ–‡æœ¬é•¿åº¦æ¥è¿‘æˆ–è¾¾åˆ°ä¸Šé™ {max_chars} å­—ç¬¦..."); limit_warning_shown=True; break
                                    combined_text+=post_content; current_chars+=len(post_content); posts_included_count+=1; comment_count=0; comments_text=""
                                    for comment in post.get('comments',[]):
                                         if isinstance(comment,dict): comment_content=f"  è¯„è®º (ID: {comment.get('id','')}, Score: {comment.get('score','')}): {comment.get('body','')}\n"; potential_len=current_chars+len(comment_content);
                                         if potential_len>=max_chars:
                                                if not limit_warning_shown: st.warning(f"æ–‡æœ¬é•¿åº¦æ¥è¿‘æˆ–è¾¾åˆ°ä¸Šé™ {max_chars} å­—ç¬¦..."); limit_warning_shown=True; break
                                         comments_text+=comment_content; current_chars+=len(comment_content); comment_count+=1
                                         if comment_count>=10: break
                                    combined_text+=comments_text+"---\n"
                            full_initial_message_for_api=f"{analysis_prompt}\n\nä»¥ä¸‹æ˜¯ç›¸å…³æ•°æ®ï¼š\n\n{combined_text}"; history_for_api_call=[]
                            if chat_provider=="Gemini": history_for_api_call=[{"role":"user","parts":[full_initial_message_for_api]}]
                            elif chat_provider=="Qwen": history_for_api_call=[{"role":"system","content":analysis_prompt},{"role":"user","content":f"ä»¥ä¸‹æ˜¯ç›¸å…³æ•°æ®ï¼š\n\n{combined_text}"}]
                            st.write(f"æœ€ç»ˆå‡†å¤‡å‘é€ç»™ {model_display_name_for_this_session} çš„æ–‡æœ¬åŒ…å« {posts_included_count} ä¸ªå¸–å­ã€‚æ€»å­—ç¬¦æ•°çº¦: {current_chars}"); st.session_state.chat_context["initial_input"]=full_initial_message_for_api
                        with st.spinner(f"æ­£åœ¨è°ƒç”¨ {model_display_name_for_this_session} è¿›è¡Œåˆæ­¥åˆ†æ..."): # Call Initial Analysis
                            initial_response_text=generate_ai_response(active_chat_key, history_for_api_call, model_details_for_this_session, active_chat_client)
                            st.session_state.chat_context["history"]=[]; st.session_state.chat_context["history"].append({"role":"user","parts":[analysis_prompt]}); st.session_state.chat_context["history"].append({"role":"model","parts":[initial_response_text]}); st.rerun()
                if "history" in st.session_state.chat_context and isinstance(st.session_state.chat_context["history"], list) and st.session_state.chat_context["history"]: # Display Chat History
                    st.markdown("---"); st.write(f"**ä¸ {model_display_name_for_this_session} çš„å¯¹è¯åˆ†æè®°å½•:**")
                    for message in st.session_state.chat_context["history"]:
                         if isinstance(message,dict) and"role"in message and"parts"in message and isinstance(message["parts"],list) and message["parts"]: role_disp=message["role"];
                         with st.chat_message(role_disp): st.markdown(message["parts"][0])
                chat_input_disabled=not(active_chat_client and active_chat_key); # Chat Input Box
                if prompt := st.chat_input(f"å‘ {model_display_name_for_this_session} ç»§ç»­æé—®...", disabled=chat_input_disabled, key="chat_input"):
                     st.session_state.chat_context["history"].append({"role":"user","parts":[prompt]});
                     with st.chat_message("user"): st.markdown(prompt)
                     with st.chat_message("model"):
                         with st.spinner(f"{model_display_name_for_this_session} æ­£åœ¨æ€è€ƒ..."):
                             response=generate_ai_response(active_chat_key, st.session_state.chat_context["history"], model_details_for_this_session, active_chat_client)
                             if isinstance(response,str) and not response.startswith("é”™è¯¯ï¼š"): st.markdown(response); st.session_state.chat_context["history"].append({"role":"model","parts":[response]})
                             else: st.error(f"æœªèƒ½ä» {model_display_name_for_this_session} è·å–æœ‰æ•ˆå›å¤ã€‚é”™è¯¯: {response}");
                             if st.session_state.chat_context["history"] and st.session_state.chat_context["history"][-1]["role"]=="user": st.session_state.chat_context["history"].pop()
                     st.rerun()
             else: st.error(f"æ— æ³•ä¸ºæ¨¡å‹ {model_display_name_for_this_session} åˆå§‹åŒ–å®¢æˆ·ç«¯æˆ–è·å– API Keyï¼Œæ— æ³•è¿›è¡ŒèŠå¤©ã€‚")

# --- Fallback Message ---
elif start_scrape_button and st.session_state.scraped_data is None: st.error("æœªèƒ½æˆåŠŸæŠ“å–æ•°æ®ï¼Œè¯·æ£€æŸ¥ä¾§è¾¹æ é…ç½®ã€Reddit API çŠ¶æ€æˆ–ç½‘ç»œè¿æ¥ã€‚")
elif not start_scrape_button and st.session_state.scraped_data is None: st.info("è¯·åœ¨å·¦ä¾§é…ç½®å‚æ•°å¹¶ç‚¹å‡»â€œå¼€å§‹æŠ“å– Reddit æ•°æ®â€æŒ‰é’®ã€‚")

# --- Footer ---
st.markdown("---")
st.caption("Reddit åˆ†æå·¥å…· v14 - ä½¿ç”¨ Streamlit, PRAW, Google Generative AI, Qwen3 æ„å»º")
# ----- END OF reddit_analyzer_cloud_app_v14_syntax_fix_2.py -----