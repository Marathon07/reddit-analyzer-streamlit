# ----- BEGINNING OF reddit_analyzer_cloud_app.py -----
# Final Cloud Version v6 - Ensuring consistent space indentation
import streamlit as st
import praw
import json
from datetime import datetime, time
import pandas as pd
import time as py_time
import pytz
import os
import google.generativeai as genai

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="Reddit åˆ†æå™¨ + AI å¯¹è¯", layout="wide", initial_sidebar_state="expanded")

# --- æ·»åŠ å›¾æ ‡ ---
st.title(":mag: Reddit åˆ†æå™¨ + AI å¯¹è¯ :robot_face:")
st.caption("æŠ“å– Reddit è®¨è®ºï¼Œå¹¶é€šè¿‡å¤šè½®å¯¹è¯ä¸ Gemini AI è¿›è¡Œåˆ†æ")

# === Gemini å…³é”®è¯å¤„ç†å‡½æ•° ===
def get_english_keywords_for_query(api_key_used, user_query):
    """ä½¿ç”¨ Gemini æ£€æµ‹è¯­è¨€ï¼Œç¿»è¯‘ä¸­æ–‡ï¼Œå¹¶æ‰©å±•è‹±æ–‡å…³é”®è¯"""
    if not api_key_used:
        st.warning("æœªé…ç½® Gemini Keyï¼Œæ— æ³•è‡ªåŠ¨å¤„ç†ä¸­æ–‡å…³é”®è¯ã€‚å°†ç›´æ¥ä½¿ç”¨åŸå§‹è¾“å…¥ã€‚")
        return user_query
    if not user_query.strip():
         st.warning("è¾“å…¥çš„å…³é”®è¯ä¸ºç©ºï¼Œæ— æ³•å¤„ç†ã€‚")
         return ""
    try:
        model = genai.GenerativeModel('gemini-2.5-pro-exp-03-25') # ä½¿ç”¨å…è´¹å®éªŒæ¨¡å‹
        prompt = f"""
        Analyze the following user query for a Reddit search: "{user_query}"

        1. Detect the primary language of the query.
        2. If the language is Chinese, translate the core meaning into a concise English keyword or phrase. Also generate 2-3 additional relevant English synonyms or related search terms.
        3. If the language is already English, use the original query as the primary keyword and generate 2-3 additional relevant English synonyms or related search terms based on the core meaning.
        4. Return ONLY a comma-separated list of the final English keywords/phrases (the primary one first, followed by related terms). Ensure each term is relevant for a keyword search. Example output for "è€å¹´äºº æ¶ˆè´¹": "(elderly consumption), (senior spending), (aging population purchase), (geriatric cost)"
        """
        response = model.generate_content(prompt)
        if hasattr(response, 'text') and response.text:
            keywords_str = response.text.strip()
            keywords_list = [kw.strip() for kw in keywords_str.split(',') if kw.strip()]
            if keywords_list:
                if len(keywords_list) == 1:
                    term = keywords_list[0].strip('()')
                    reddit_query = f"({term})" if term else ""
                else:
                    processed_terms = []
                    for kw in keywords_list:
                         term = kw.strip('()')
                         if term: processed_terms.append(f"({term})")
                    reddit_query = " OR ".join(processed_terms) if processed_terms else ""
                original_term_in_parentheses = f"({user_query.strip()})"
                if reddit_query and reddit_query.lower() != original_term_in_parentheses.lower():
                     st.info(f"å·²å°†è¾“å…¥è‡ªåŠ¨è½¬æ¢ä¸º/æ‰©å±•ä¸ºè‹±æ–‡ OR æŸ¥è¯¢: {reddit_query}")
                elif reddit_query:
                     st.info("è¾“å…¥ä¸ºè‹±æ–‡æˆ–å·²å¤„ç†ï¼Œå°†ä½¿ç”¨ä»¥ä¸‹æŸ¥è¯¢ã€‚")
                return reddit_query if reddit_query else user_query
            else:
                st.warning("Gemini æœªèƒ½æå–æœ‰æ•ˆçš„è‹±æ–‡å…³é”®è¯...")
                return user_query
        else:
            st.warning(f"Geminiæœªèƒ½å¤„ç†å…³é”®è¯...")
            return user_query
    except Exception as e:
        st.error(f"è°ƒç”¨ Gemini å¤„ç†å…³é”®è¯æ—¶å‡ºé”™: {e}")
        return user_query

# === æ ¸å¿ƒæŠ“å–å‡½æ•° ===
def run_scraper(sub_name, query, limit, t_filter, get_comments, use_dates, start_dt, end_dt):
    """æ‰§è¡Œ Reddit æ•°æ®æŠ“å–çš„å‡½æ•° (äº‘ç‰ˆæœ¬ - ä¸è·³è¿‡å·²å¤„ç†å¸–å­)"""
    all_data = []
    status_area = st.empty()
    try:
        status_area.info("æ­£åœ¨åˆå§‹åŒ– PRAW å¹¶è¿æ¥ Reddit API...")
        # !! ä½¿ç”¨ä¾§è¾¹æ è¯»å–çš„å…¨å±€å˜é‡ !!
        reddit = praw.Reddit(client_id=REDDIT_CLIENT_ID, client_secret=REDDIT_CLIENT_SECRET, user_agent=REDDIT_USER_AGENT, read_only=True)
        subreddit = reddit.subreddit(sub_name)
        status_area.info(f"è¿æ¥æˆåŠŸï¼æ­£åœ¨ r/{sub_name} ä¸­æœç´¢ '{query}' (åŸºæœ¬æ—¶é—´èŒƒå›´: {t_filter})...")
        if not query or not query.strip('()'):
            status_area.warning("æœç´¢æŸ¥è¯¢ä¸ºç©º...")
            return []
        search_results = list(subreddit.search(query=query, sort='new', time_filter=t_filter, limit=limit))
        status_area.info(f"åˆæ­¥æ‰¾åˆ° {len(search_results)} ä¸ªå¸–å­...")
        start_timestamp, end_timestamp = None, None
        if use_dates and start_dt and end_dt:
            try:
                start_datetime_naive = datetime.combine(start_dt, time.min); end_datetime_naive = datetime.combine(end_dt, time.max)
                utc_tz = pytz.utc; start_timestamp = utc_tz.localize(start_datetime_naive).timestamp(); end_timestamp = utc_tz.localize(end_datetime_naive).timestamp()
                if start_timestamp > end_timestamp: st.warning("å¼€å§‹æ—¥æœŸæ™šäºç»“æŸæ—¥æœŸ..."); start_timestamp, end_timestamp = None, None
                else: status_area.info(f"å°†åº”ç”¨æ—¥æœŸèŒƒå›´: {start_dt} è‡³ {end_dt}")
            except Exception as date_e: st.error(f"å¤„ç†æ—¥æœŸè¾“å…¥æ—¶å‡ºé”™: {date_e}"); start_timestamp, end_timestamp = None, None
        total_posts_processed_this_run, date_skipped_count = 0, 0
        progress_bar = st.progress(0) if search_results else None
        for i, submission in enumerate(search_results):
            if progress_bar: progress_bar.progress((i + 1) / len(search_results))
            if start_timestamp and end_timestamp:
                if not (start_timestamp <= submission.created_utc <= end_timestamp): date_skipped_count +=1; continue
            status_area.info(f"æ­£åœ¨å¤„ç†å¸–å­ {i+1}/{len(search_results)}: {submission.id} - {submission.title[:30]}...")
            post_info = {'id': submission.id, 'title': submission.title, 'selftext': submission.selftext, 'url': submission.url, 'score': submission.score, 'created_utc': submission.created_utc, 'comments': []}
            if get_comments:
                try:
                    submission.comment_sort = 'new'; submission.comments.replace_more(limit=0)
                    for comment in submission.comments.list():
                        if hasattr(comment, 'body'): post_info['comments'].append({'id': comment.id, 'author': str(comment.author),'body': comment.body, 'score': comment.score,'created_utc': comment.created_utc, 'parent_id': comment.parent_id, 'depth': comment.depth})
                except Exception as e: st.warning(f"å¤„ç†å¸–å­ {submission.id} è¯„è®ºæ—¶å‡ºé”™: {e}")
            all_data.append(post_info); total_posts_processed_this_run += 1; py_time.sleep(0.1)
        status_area.success(f"æŠ“å–å®Œæˆï¼æœ¬æ¬¡è¿è¡Œå¤„ç†äº† {total_posts_processed_this_run} ä¸ªå¸–å­ã€‚è·³è¿‡äº† {date_skipped_count} ä¸ªä¸ç¬¦åˆæ—¥æœŸèŒƒå›´çš„å¸–å­ã€‚")
        return all_data
    except praw.exceptions.PRAWException as pe: status_area.error(f"Reddit API (PRAW) é”™è¯¯: {pe}"); return None
    except Exception as e: status_area.error(f"å‘ç”Ÿæ„å¤–é”™è¯¯: {e}"); return None

# === Gemini åˆ†æå‡½æ•° ===
def generate_gemini_response(api_key_used, chat_history):
    """è°ƒç”¨ Gemini API ç”ŸæˆèŠå¤©å›å¤"""
    if not api_key_used:
        return "é”™è¯¯ï¼šæœªé…ç½® Gemini API Keyã€‚"
    if not chat_history:
        return "é”™è¯¯ï¼šèŠå¤©å†å²ä¸ºç©ºã€‚"
    try:
        model = genai.GenerativeModel('gemini-2.5-pro-exp-03-25') # ä½¿ç”¨å…è´¹å®éªŒæ¨¡å‹
        response = model.generate_content(chat_history)
        if hasattr(response, 'text'):
             return response.text
        else:
             feedback = getattr(response, 'prompt_feedback', 'æ— è¯¦ç»†åé¦ˆ')
             st.error(f"Gemini API è¿”å›äº†å“åº”ï¼Œä½†æ— æ³•æå–æ–‡æœ¬ã€‚å¯èƒ½æ˜¯å› ä¸ºå†…å®¹å®‰å…¨é˜»æ­¢ã€‚åé¦ˆ: {feedback}")
             return f"é”™è¯¯ï¼šGemini API è¿”å›äº†æ— æ•ˆå“åº”æˆ–è¢«é˜»æ­¢ã€‚åé¦ˆ: {feedback}"
    except Exception as e:
        st.error(f"è°ƒç”¨ Gemini API æ—¶å‡ºé”™: {e}")
        return f"è°ƒç”¨ Gemini API æ—¶å‡ºé”™: {e}"

# --- ä¾§è¾¹æ å®šä¹‰ ---
with st.sidebar:
    st.header("å‚æ•°é…ç½®")
    # --- API å‡­è¯è¯»å– ---
    # Reddit
    try:
        REDDIT_CLIENT_ID = st.secrets["REDDIT_CLIENT_ID"]
        REDDIT_CLIENT_SECRET = st.secrets["REDDIT_CLIENT_SECRET"]
        REDDIT_USER_AGENT = st.secrets["REDDIT_USER_AGENT"]
        if not REDDIT_CLIENT_ID or not REDDIT_CLIENT_SECRET or not REDDIT_USER_AGENT:
             st.error("é”™è¯¯ï¼šReddit API å‡­è¯æœªé…ç½®..."); st.stop()
    except KeyError: st.error("é”™è¯¯ï¼šè¯·é…ç½® Reddit API å‡­è¯ Secretsã€‚"); st.stop()
    except Exception as e: st.error(f"è¯»å– Reddit Secrets æ—¶å‡ºé”™: {e}"); st.stop()
    # Gemini
    GEMINI_API_KEY = None; gemini_configured_successfully = False
    try:
        GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
        if not GEMINI_API_KEY: st.warning("æç¤ºï¼šGemini API Key æœªé…ç½®...")
        else:
            try: genai.configure(api_key=GEMINI_API_KEY); st.success("Gemini API Key é…ç½®æˆåŠŸï¼"); gemini_configured_successfully = True
            except Exception as gemini_config_e: st.error(f"é…ç½® Gemini API æ—¶å‡ºé”™: {gemini_config_e}"); GEMINI_API_KEY = None
    except KeyError: st.warning("æç¤ºï¼šæœªæ‰¾åˆ° Gemini API Key Secret...")
    except Exception as e: st.error(f"è¯»å– Gemini Secrets æ—¶å‡ºé”™: {e}")

    # --- è¾“å…¥å‚æ•° ---
    subreddit_name = st.text_input("Subreddit åç§°", "askSingapore")
    search_query_input = st.text_input("æœç´¢å…³é”®è¯ (æ”¯æŒä¸­æ–‡)", "è€å¹´äºº æ¶ˆè´¹")
    post_limit = st.number_input("æœ€å¤§å¸–å­æ•°é‡", min_value=1, max_value=1000, value=20)
    time_filter_options = {"è¿‡å»ä¸€å°æ—¶": "hour", "è¿‡å»ä¸€å¤©": "day", "è¿‡å»ä¸€å‘¨": "week", "è¿‡å»ä¸€æœˆ": "month", "è¿‡å»ä¸€å¹´": "year", "æ‰€æœ‰æ—¶é—´": "all"}
    selected_time_label = st.selectbox("åŸºæœ¬æ—¶é—´èŒƒå›´:", options=list(time_filter_options.keys()), index=5)
    time_filter = time_filter_options[selected_time_label]
    st.markdown("---")
    use_specific_dates = st.checkbox("ä½¿ç”¨ç²¾ç¡®æ—¥æœŸèŒƒå›´")
    start_date_input = st.date_input("å¼€å§‹æ—¥æœŸ", value=None, disabled=not use_specific_dates)
    end_date_input = st.date_input("ç»“æŸæ—¥æœŸ", value=None, disabled=not use_specific_dates)
    st.caption("*æ—¥æœŸç­›é€‰åœ¨è·å–å¸–å­åè¿›è¡Œã€‚*")
    st.markdown("---")
    fetch_comments = st.checkbox("æŠ“å–è¯„è®º (å¯èƒ½å¾ˆæ…¢!)", value=True)
    st.markdown("---")
    # --- è§¦å‘æŠ“å–æŒ‰é’® ---
    start_scrape_button = st.button("å¼€å§‹æŠ“å– Reddit æ•°æ®", type="primary", icon="ğŸ”")

# --- åˆå§‹åŒ– Session State ---
if 'scraped_data' not in st.session_state: st.session_state.scraped_data = None
if 'chat_history' not in st.session_state: st.session_state.chat_history = []
if 'initial_api_input' not in st.session_state: st.session_state.initial_api_input = None
if 'final_query_used' not in st.session_state: st.session_state.final_query_used = None

# --- ä¸»é€»è¾‘ï¼šå¤„ç†æŠ“å–æŒ‰é’®ç‚¹å‡» ---
if start_scrape_button:
    st.session_state.scraped_data = None; st.session_state.chat_history = []; st.session_state.initial_api_input = None; st.session_state.final_query_used = None
    st.info(f"æ”¶åˆ°æŠ“å–ä»»åŠ¡ï¼Œæ­£åœ¨å¤„ç†å…³é”®è¯ '{search_query_input}'...")
    final_reddit_query = search_query_input
    if gemini_configured_successfully:
        with st.spinner("æ­£åœ¨ä½¿ç”¨ AI å¤„ç†/ç¿»è¯‘å…³é”®è¯..."): final_reddit_query = get_english_keywords_for_query(GEMINI_API_KEY, search_query_input)
    st.session_state.final_query_used = final_reddit_query
    if st.session_state.final_query_used: st.info(f"æœ€ç»ˆç”¨äº Reddit æœç´¢çš„æŸ¥è¯¢è¯­å¥: '{st.session_state.final_query_used}'")
    else: st.warning("å…³é”®è¯å¤„ç†åä¸ºç©º..."); st.stop()
    with st.spinner(f"æ­£åœ¨ r/{subreddit_name} ä¸­æœç´¢å¹¶æŠ“å–æ•°æ®..."):
        scraped_data_result = run_scraper(subreddit_name, st.session_state.final_query_used, post_limit, time_filter, fetch_comments, use_specific_dates, start_date_input, end_date_input)
    st.session_state.scraped_data = scraped_data_result
    st.rerun()

# --- æ˜¾ç¤ºæŠ“å–ç»“æœå’Œå¯åŠ¨åˆ†æ/å¯¹è¯ ---
# (æ³¨æ„è¿™é‡Œçš„ if/elif/else ç»“æ„å’Œç¼©è¿›)
if st.session_state.scraped_data is not None:
    # --- åˆ›å»º Tabs ---
    tab_preview, tab_ai_chat = st.tabs(["ğŸ“Š æ•°æ®é¢„è§ˆä¸ä¸‹è½½", "ğŸ’¬ AI å¯¹è¯åˆ†æ"])

    with tab_preview:
        st.subheader("æŠ“å–ç»“æœé¢„è§ˆ (æœ¬æ¬¡è¿è¡Œæ‰¾åˆ°çš„æ‰€æœ‰å¸–å­)")
        st.write(f"å…±æ‰¾åˆ° {len(st.session_state.scraped_data)} ä¸ªç¬¦åˆæ¡ä»¶çš„å¸–å­è®°å½•ã€‚")
        if st.session_state.final_query_used: st.write(f"(ä½¿ç”¨çš„æœ€ç»ˆæœç´¢æŸ¥è¯¢: `{st.session_state.final_query_used}`) ")
        if not st.session_state.scraped_data: st.warning("æœ¬æ¬¡è¿è¡Œæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å¸–å­ã€‚")
        else:
            # --- é¢„è§ˆè¡¨æ ¼ ---
            try:
                posts_df_data = [{k: v for k, v in post.items() if k != 'comments'} for post in st.session_state.scraped_data]
                posts_df = pd.DataFrame(posts_df_data)
                if not posts_df.empty and 'created_utc' in posts_df.columns:
                    posts_df['created_datetime_sgt'] = pd.to_datetime(posts_df['created_utc'], unit='s', utc=True).dt.tz_convert('Asia/Singapore')
                    preview_cols = ['title', 'score', 'created_datetime_sgt', 'url', 'id', 'selftext']; display_cols = [col for col in preview_cols if col in posts_df.columns]
                    st.dataframe(posts_df[display_cols])
                else: st.dataframe(posts_df)
            except Exception as e: st.warning(f"æ— æ³•å°†ç»“æœæ ¼å¼åŒ–ä¸ºè¡¨æ ¼æ˜¾ç¤º: {e}")
            # --- ä¸‹è½½æŒ‰é’® ---
            st.subheader("ä¸‹è½½æœ¬æ¬¡æŠ“å–çš„æ•°æ® (JSON)")
            try:
                json_string = json.dumps(st.session_state.scraped_data, indent=4, ensure_ascii=False)
                timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
                safe_query = "".join(c if c.isalnum() else "_" for c in (st.session_state.final_query_used or search_query_input))[:50]
                download_filename = f"reddit_{subreddit_name}_search_{safe_query}_{timestamp_str}.json"
                st.download_button(label="ä¸‹è½½ JSON æ•°æ®", data=json_string, file_name=download_filename, mime="application/json")
            except Exception as e: st.error(f"ç”Ÿæˆ JSON ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    with tab_ai_chat:
        st.subheader("ä¸ Gemini AI å¯¹è¯åˆ†æ")
        if not gemini_configured_successfully:
            st.error("æœªæˆåŠŸé…ç½® Gemini API Keyï¼Œæ— æ³•è¿›è¡Œ AI å¯¹è¯ã€‚")
        else:
            # --- æŒä¹…æ˜¾ç¤ºé¦–æ¬¡åˆ†æè¾“å…¥ Expander ---
            if st.session_state.initial_api_input:
                with st.expander("æŸ¥çœ‹é¦–æ¬¡åˆ†æå‘é€ç»™ Gemini çš„å®Œæ•´è¾“å…¥ (æŒ‡ä»¤+æ•°æ®)", expanded=False): st.text(st.session_state.initial_api_input)
                st.markdown("---")
            # --- åˆå§‹åˆ†æ Prompt è¾“å…¥å’Œè§¦å‘æŒ‰é’® ---
            if not st.session_state.chat_history:
                query_for_prompt = st.session_state.final_query_used or search_query_input
                default_initial_prompt = f"ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æŠ“å–çš„å…³äº '{query_for_prompt}' çš„ Reddit å¸–å­å’Œè¯„è®ºå†…å®¹ï¼Œæ€»ç»“ä¸»è¦çš„è®¨è®ºè§‚ç‚¹ã€ç”¨æˆ·æƒ…ç»ªã€å¯èƒ½å­˜åœ¨çš„æ¶ˆè´¹è¶‹åŠ¿ä»¥åŠäºšé©¬é€Šæ–°åŠ å¡ç«™çš„é€‰å“æ€è·¯ï¼Œå¹¶ç”¨è¡¨æ ¼åˆ—å‡ºæ¯ä¸ªé€‰å“ä¸Redditå†…å®¹çš„å…³è”æ€§ï¼Œå¹¶æŒ‰ç…§ç”¨æˆ·éœ€æ±‚çš„è¿«åˆ‡æ€§å¯¹é€‰å“è¿›è¡Œæ’åºã€‚"
                st.write("**åˆæ­¥åˆ†ææŒ‡ä»¤ (å¯ç¼–è¾‘):**")
                analysis_prompt_input = st.text_area(label="åˆæ­¥åˆ†ææŒ‡ä»¤ (å¯ç¼–è¾‘):", value=default_initial_prompt, height=150, key="initial_prompt_input_area", label_visibility="collapsed")
                if st.button("è¿›è¡Œåˆæ­¥ AI åˆ†æ", icon=":material/auto_awesome:"):
                    with st.spinner("æ­£åœ¨å‡†å¤‡åˆæ­¥åˆ†æè¯·æ±‚..."):
                        analysis_prompt = analysis_prompt_input; combined_text = ""; max_chars = 3000000; current_chars = 0; posts_included_count = 0
                        # --- å¡«å…… combined_text çš„å¾ªç¯ ---
                        for post in st.session_state.scraped_data:
                            post_content = f"å¸–å­ ID: {post.get('id', '')}\næ ‡é¢˜: {post.get('title', '')}\næ­£æ–‡: {post.get('selftext', '')}\n"
                            if current_chars + len(post_content) >= max_chars: st.warning(f"æ–‡æœ¬é•¿åº¦è¾¾åˆ°ä¸Šé™ {max_chars}..."); limit_warning_shown = True; break # Ensure flag logic is here if used
                            combined_text += post_content; current_chars += len(post_content); posts_included_count += 1; comment_count = 0; comments_text = ""
                            for comment in post.get('comments', []):
                                comment_content = f"  è¯„è®º (ID: {comment.get('id','')}, Score: {comment.get('score', '')}): {comment.get('body', '')}\n"
                                if current_chars + len(comment_content) >= max_chars: # Use >= for consistency
                                    # if not limit_warning_shown: # Add flag if single warning desired
                                    st.warning(f"æ–‡æœ¬é•¿åº¦è¾¾åˆ°ä¸Šé™ {max_chars}..."); limit_warning_shown = True
                                    break
                                comments_text += comment_content; current_chars += len(comment_content); comment_count += 1
                                if comment_count >= 10: break
                            combined_text += comments_text + "---\n"
                        # ----------------------------------
                        full_initial_message_for_api = f"{analysis_prompt}\n\nä»¥ä¸‹æ˜¯ç›¸å…³æ•°æ®ï¼š\n\n{combined_text}"
                        history_for_api_call = [{"role": "user", "parts": [full_initial_message_for_api]}]
                        st.write(f"æœ€ç»ˆå‡†å¤‡å‘é€ç»™ Gemini çš„æ–‡æœ¬åŒ…å« {posts_included_count} ä¸ªå¸–å­ï¼ˆéƒ¨åˆ†è¯„è®ºå¯èƒ½å› é•¿åº¦é™åˆ¶è¢«æˆªæ–­ï¼‰ã€‚")
                        st.session_state.initial_api_input = full_initial_message_for_api # å­˜å‚¨é¦–æ¬¡è¾“å…¥
                    with st.spinner("æ­£åœ¨è°ƒç”¨ Gemini è¿›è¡Œåˆæ­¥åˆ†æ..."):
                        initial_response_text = generate_gemini_response(GEMINI_API_KEY, history_for_api_call)
                        st.session_state.chat_history = [] # æ¸…ç©ºæ—§å†å²è®°å½•
                        st.session_state.chat_history.append({"role": "user", "parts": [analysis_prompt]}) # åªå­˜æŒ‡ä»¤
                        st.session_state.chat_history.append({"role": "model", "parts": [initial_response_text]}) # å­˜å›å¤
                        st.rerun() # Rerun ä»¥æ˜¾ç¤ºèŠå¤©è®°å½•å’ŒæŒä¹…çš„ Expander
            # --- æ˜¾ç¤ºèŠå¤©è®°å½• ---
            if st.session_state.chat_history:
                st.markdown("---"); st.write("**AI å¯¹è¯åˆ†æè®°å½•:**")
                for message in st.session_state.chat_history:
                     with st.chat_message(message["role"]): st.markdown(message["parts"][0])
            # --- èŠå¤©è¾“å…¥æ¡† ---
            if prompt := st.chat_input("å°±åˆ†æç»“æœæˆ–æ•°æ®è¿›è¡Œæé—®...", disabled=not gemini_configured_successfully):
                 st.session_state.chat_history.append({"role": "user", "parts": [prompt]})
                 with st.chat_message("user"): st.markdown(prompt)
                 with st.chat_message("model"):
                     with st.spinner("Gemini æ­£åœ¨æ€è€ƒ..."):
                         response = generate_gemini_response(GEMINI_API_KEY, st.session_state.chat_history)
                         if isinstance(response, str) and not response.startswith("é”™è¯¯ï¼š"):
                             st.markdown(response); st.session_state.chat_history.append({"role": "model", "parts": [response]})
                         else:
                             if st.session_state.chat_history and st.session_state.chat_history[-1]["role"] == "user": st.session_state.chat_history.pop() # Pop user message if AI fails
                 st.rerun()

elif start_scrape_button: # <--- æ£€æŸ¥è¿™é‡Œçš„å¯¹é½
    st.error("æœªèƒ½æˆåŠŸæŠ“å–æ•°æ®ï¼Œè¯·æ£€æŸ¥ä¾§è¾¹æ é…ç½®æˆ– API çŠ¶æ€ã€‚")
else: # <--- æ£€æŸ¥è¿™é‡Œçš„å¯¹é½
    st.write("è¯·åœ¨å·¦ä¾§é…ç½®å‚æ•°å¹¶ç‚¹å‡»â€œå¼€å§‹æŠ“å– Reddit æ•°æ®â€æŒ‰é’®ã€‚")

# --- é¡µè„š ---
st.markdown("---")
st.caption("ä¸€ä¸ªä½¿ç”¨ Streamlit å’Œ PRAW æ„å»ºçš„ Reddit åˆ†æå·¥å…· (Cloud + AI Chat + Keyword Processing + Tabs/Icons)")
# ----- END OF reddit_analyzer_cloud_app.py -----